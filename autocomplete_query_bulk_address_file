# Script Name: Address Autocomplete Processor
# Version: 2.0 (add csv input)

# !pip install nest_asyncio  # This should be run as a separate cell in Jupyter

# Import section
import requests
import csv
import json
import pandas as pd
import logging
import os
from google.colab import files, userdata
from typing import List, Dict, Any, Optional, Tuple  # Added Optional AND Tuple here
import pytz
from datetime import datetime, timezone
from tqdm import tqdm
import pickle
from pathlib import Path
import asyncio
import aiohttp
import sys
import nest_asyncio

# 1. Configuration and Setup
# 1.1 Constants and API configuration
API_URL = "https://api.realestateapi.com/v2/AutoComplete"
MAX_RETRIES = 3
DELAY_BETWEEN_CALLS = 0.01  # seconds
MAX_CONCURRENT_REQUESTS = 10  # Adjust based on API rate limits

# 1.2 Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# 1.3 Check Colab files
def list_files(file_types: tuple) -> List[str]:
    return [f for f in os.listdir() if f.lower().endswith(file_types)]

# 1.3.2 List available files and handle file selection
def select_file() -> str:
    file_types = ('.txt', '.json', '.csv')
    available_files = list_files(file_types)
    
    if not available_files:
        print("No .txt, .json, or .csv files found in the current directory.")
        print("1. Upload a new file")
    else:
        print("Available files:")
        for i, file in enumerate(available_files, 1):
            print(f"{i}. {file}")
        print(f"{len(available_files) + 1}. Upload a new file")
    
    while True:
        try:
            choice = int(input("Enter the number of your choice: "))
            if 1 <= choice <= len(available_files):
                selected_file = available_files[choice - 1]
                print(f"Selected file: {selected_file}")
                return selected_file
            elif choice == len(available_files) + 1 or (not available_files and choice == 1):
                # Upload a new file
                print("Please upload a file.")
                uploaded = files.upload()
                if uploaded:
                    return list(uploaded.keys())[0]
                print("No file was uploaded. Please try again.")
            else:
                print("Invalid choice. Please try again.")
        except ValueError:
            print("Invalid input. Please enter a number.")
        except EOFError:
            print("Error reading input. Please try again.")
        except Exception as e:
            print(f"An error occurred: {str(e)}. Please try again.")

# 1.4 Caching setup
CACHE_FILE = 'address_cache.pkl'

def load_cache():
    if Path(CACHE_FILE).exists():
        with open(CACHE_FILE, 'rb') as f:
            return pickle.load(f)
    return {}

def save_cache(cache):
    with open(CACHE_FILE, 'wb') as f:
        pickle.dump(cache, f)

address_cache = load_cache()

# 2. Helper Functions
# 2.1 API key retrieval and caching
api_key_cache: Optional[str] = None

def get_api_key() -> str:
    global api_key_cache
    if api_key_cache is None:
        api_key_cache = userdata.get('x-api-key')
        if not api_key_cache:
            logger.warning("API key not found in Colab secrets. Prompting user for manual input.")
            api_key_cache = input("Please enter your API key: ")
        if not api_key_cache:
            raise ValueError("API key is required to proceed.")
    return api_key_cache

# 2.2 API request function
async def make_api_request(session: aiohttp.ClientSession, address: str) -> Dict[str, Any]:
    headers = {
        "x-user-id": "UniqueUserIdentifier",
        "content-type": "application/json",
        "x-api-key": get_api_key()
    }
    payload = {
        "search_types": ["A"],
        "search": address
    }
    for attempt in range(MAX_RETRIES):
        try:
            async with session.post(API_URL, json=payload, headers=headers) as response:
                response.raise_for_status()
                return await response.json()
        except aiohttp.ClientResponseError as e:
            logger.warning(f"HTTP error (attempt {attempt + 1}/{MAX_RETRIES}): {e.status} - {e.message}")
        except aiohttp.ClientError as e:
            logger.warning(f"Client error (attempt {attempt + 1}/{MAX_RETRIES}): {str(e)}")
        except asyncio.TimeoutError:
            logger.warning(f"Timeout error (attempt {attempt + 1}/{MAX_RETRIES})")
        except Exception as e:
            logger.warning(f"Unexpected error (attempt {attempt + 1}/{MAX_RETRIES}): {str(e)}")
        await asyncio.sleep(DELAY_BETWEEN_CALLS * (attempt + 1))
    logger.error(f"Max retries reached for address: {address}")
    return {}

# 2.3 Process addresses
async def process_addresses(addresses: List[Tuple[str, str]]) -> List[Dict[str, Any]]:
    results = []
    async with aiohttp.ClientSession() as session:
        semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
        tasks = []
        for original, address in addresses:
            if address in address_cache:
                result = address_cache[address]
                result['input_address'] = original
                results.append(result)
            else:
                task = asyncio.ensure_future(process_single_address(session, semaphore, original, address))
                tasks.append(task)
        
        for future in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="Processing addresses"):
            try:
                result = await future
                results.append(result)
            except Exception as e:
                logger.error(f"Error processing address: {str(e)}")
    
    save_cache(address_cache)
    return results

# 2.3.1 Process single address
async def process_single_address(session: aiohttp.ClientSession, semaphore: asyncio.Semaphore, original: str, address: str) -> Dict[str, Any]:
    async with semaphore:
        try:
            result = await make_api_request(session, address)
            data = result.get('data', [])
            if data:
                data = data[0]
            else:
                data = {}
            
            data['input_address'] = original
            
            if not data.get('address'):
                data['error'] = 'No data returned'
            
            address_cache[address] = data
            await asyncio.sleep(DELAY_BETWEEN_CALLS)
            return data
        except Exception as e:
            logger.error(f"Error processing address '{address}': {str(e)}")
            return {"input_address": original, "error": str(e)}

# 2.4 Convert JSON results to DataFrame
def json_to_dataframe(results: List[Dict[str, Any]]) -> pd.DataFrame:
    flattened_data = []
    for item in results:
        flattened_item = {
            'input_address': item.get('input_address', ''),
            'id': item.get('id', ''),
            'title': item.get('title', ''),
            'address': item.get('address', ''),
            'city': item.get('city', ''),
            'state': item.get('state', ''),
            'zip': item.get('zip', ''),
            'county': item.get('county', ''),
            'latitude': item.get('latitude', ''),
            'longitude': item.get('longitude', ''),
            'type': item.get('type', ''),
            'error': item.get('error', '')
        }
        flattened_data.append(flattened_item)
    return pd.DataFrame(flattened_data)

# 2.5 Address parsing function
def parse_addresses(file_name: str) -> List[Tuple[str, str]]:
    addresses = []
    file_extension = os.path.splitext(file_name)[1].lower()

    if file_extension == '.json':
        with open(file_name, 'r') as f:
            for line in f:
                try:
                    line = line.strip().rstrip(',')
                    parsed = json.loads(line)
                    original = parsed['address']
                    addresses.append((original, original))
                except json.JSONDecodeError:
                    logger.warning(f"Invalid JSON in line: {line}")
                except KeyError:
                    logger.warning(f"Missing 'address' key in line: {line}")
                except Exception as e:
                    logger.warning(f"Unexpected error parsing line: {line}. Error: {str(e)}")
    elif file_extension in ['.csv', '.txt']:
        with open(file_name, 'r', newline='') as f:
            reader = csv.reader(f)
            headers = next(reader, None)
            
            if headers and len(headers) > 1:
                print("Multiple columns detected. Please select the column containing addresses:")
                for i, header in enumerate(headers):
                    print(f"{i + 1}. {header}")
                
                while True:
                    try:
                        choice = int(input("Enter the number of your choice: ")) - 1
                        if 0 <= choice < len(headers):
                            address_column = choice
                            break
                        else:
                            print("Invalid choice. Please try again.")
                    except ValueError:
                        print("Invalid input. Please enter a number.")
            else:
                address_column = 0
            
            for row in reader:
                if row:
                    address = row[address_column].strip()
                    addresses.append((address, address))
    
    return addresses

# 3. Main Execution
async def main_async():
    try:
        # 3.1 File selection or upload
        file_name = select_file()
        
        # 3.2 Read addresses
        addresses = parse_addresses(file_name)

        logger.info(f"Loaded {len(addresses)} addresses from {file_name}")

        # 3.3 Process addresses
        results = await process_addresses(addresses)

        # 3.4 Convert to DataFrame
        df = json_to_dataframe(results)

        if df.empty:
            logger.warning("No valid results were obtained. The output CSV will be empty.")
        elif len(df) < len(addresses):
            logger.warning(f"Only {len(df)} out of {len(addresses)} addresses were successfully processed.")

        # 3.5 Save to CSV
        est = pytz.timezone('US/Eastern')
        current_time = datetime.now(est).strftime("%m%d%y_%H%M")
        csv_filename = f'autocompleted_addresses_{current_time}.csv'
        df.to_csv(csv_filename, index=False)
        logger.info(f"Results saved to {csv_filename}")
       
        # Create the filename
        csv_filename = f'autocompleted_addresses_{current_time}.csv'

        # Save the DataFrame to CSV
        df.to_csv(csv_filename, index=False)
        logger.info(f"Results saved to {csv_filename}")

        # 3.6 Download CSV
        files.download(csv_filename)
        logger.info(f"Download initiated for {csv_filename}")

    except Exception as e:
        logger.exception(f"An unexpected error occurred: {str(e)}")

# This function will be called to run our async code
def run_main():
    nest_asyncio.apply()
    asyncio.get_event_loop().run_until_complete(main_async())

# If running in Jupyter, we'll use this cell to execute our code
if 'ipykernel' in sys.modules:
    run_main()
else:
    # For non-Jupyter environments
    if __name__ == "__main__":
        asyncio.run(main_async())
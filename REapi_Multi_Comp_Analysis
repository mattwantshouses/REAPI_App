# REapi Multi Comps Analysis (ChatGPT o1) Broken
# Version: 5.1
# Updated: 09/17/2024

# 1. Import required libraries
import sys
import subprocess

# 2. Install necessary packages and import data
def install_and_import(package):
    try:
        __import__(package)
    except ImportError:
        print(f"Installing {package}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

install_and_import('ijson')
install_and_import('pandas')
install_and_import('pytz')

# 3. Import additional libraries
import os
import json
import ijson
import pandas as pd
from datetime import datetime, timedelta
from google.colab import files
import decimal 

# 4. Define constants and field indicators based on your pattern description

# 4.1 Output fields for subject properties and comps
SUBJECT_FIELDS = [
    "label",
    "recordCount",
    "statusCode",
    "statusMessage",
    "availableEquity",
    "lastSaleDate",
    "lastSalePrice",
    "ownershipLength",
    "owner1FullName",
    "owner2FullName",
    "legalDescription",
    "subdivision",
    "apn",
    "apnUnformatted",
    "ltv",
    "mlsDaysOnMarket",
    "mlsTotalUpdates",
    "mlsActive",
    "mlsPending",
    "mlsCancelled",
    "mlsFailed",
    "mlsSold",
    "mlsListingPrice",
    "mlsListingPricePerSquareFoot",
    "mlsListingDate",
    "mlsStatus",
    "mlsType",
    "mlsLastStatusDate",
    "latitude",
    "longitude",
    "bedrooms",
    "bathrooms",
    "livingSquareFeet",
    "lotSquareFeet",
    "lotAcres",
    "yearBuilt",
    "id"
]

COMP_FIELDS = [
    "label",
    "sqFt",
    "price",
    "bedrooms",
    "bathrooms",
    "livingSquareFeet",
    "pricePerSquareFoot",
    "pricePerSqft",
    "lotSquareFeet",
    "lotAcres",
    "yearBuilt",
    "lastSaleDate",
    "lastSalePrice",
    "purchaseMethod",
    "latitude",
    "longitude",
    "id"
]

# 5. Functions to parse and process the data

# 5.1 Parse JSON stream
def parse_json_stream(file_path):
    """Parse the JSON file and yield records one by one."""
    with open(file_path, 'rb') as f:
        # Use ijson to parse multiple concatenated JSON objects
        objects = ijson.items(f, '', multiple_values=True)
        for obj in objects:
            if isinstance(obj, list):
                for item in obj:
                    yield item
            else:
                yield obj

# 5.2 Process data stream
def process_data_stream(file_path, output_file_name):
    """Process data and write to CSV in batches to handle very large files."""
    records_iterator = parse_json_stream(file_path)
    batch_size = 100
    batch = []
    columns_written = False
    total_records = 0

    def decimal_default(obj):
        if isinstance(obj, decimal.Decimal):
            return float(obj)
        raise TypeError

    with open(output_file_name, "w", newline="", encoding="utf-8") as csvfile:
        while True:
            try:
                record = next(records_iterator)
                if not isinstance(record, dict):
                    print(f"Skipping non-dictionary record: {type(record)}")
                    continue
                
                # Print the entire record for debugging
                print(f"Processing record: {json.dumps(record, indent=2, default=decimal_default)}")
                
                # Extract subject property and comps
                subject_record = {}
                comps = []

                # Assuming the record has 'comps' key
                for key, value in record.items():
                    if key == 'comps':
                        comps = value
                    else:
                        subject_record[key] = value

                if not comps:
                    # No comps, create a single record with subject fields
                    combined_record = {field: subject_record.get(field, '') for field in SUBJECT_FIELDS}
                    batch.append(combined_record)
                else:
                    for comp in comps:
                        combined_record = {field: subject_record.get(field, '') for field in SUBJECT_FIELDS}
                        comp_record = {f"comp_{field}": comp.get(field, '') for field in COMP_FIELDS}
                        combined_record.update(comp_record)
                        batch.append(combined_record)
                
                total_records += 1
                if len(batch) >= batch_size:
                    write_batch_to_csv(batch, csvfile, columns_written)
                    columns_written = True
                    batch = []
                    print(f"{total_records} records processed.")
            except StopIteration:
                # Write any remaining records
                if batch:
                    write_batch_to_csv(batch, csvfile, columns_written)
                print(f"Total records processed: {total_records}")
                break
            except Exception as e:
                print(f"Error processing record: {e}")
                # Print the problematic record
                print(f"Problematic record: {json.dumps(record, indent=2)}")
                continue

    # Check if the output file is empty
    if os.path.getsize(output_file_name) == 0:
        print("Warning: The output file is empty. No records were processed successfully.")

# 5.3 Write batch to CSV
def write_batch_to_csv(batch, csvfile, columns_written):
    """Helper function to write a batch of records to CSV."""
    df_batch = pd.DataFrame(batch)
    if df_batch.empty:
        print("Warning: Empty batch, skipping write.")
        return

    # Reorder columns
    comp_columns = [col for col in df_batch.columns if col.startswith("comp_")]
    columns_order = SUBJECT_FIELDS + comp_columns
    df_batch = df_batch[columns_order]

    if not columns_written:
        df_batch.to_csv(csvfile, index=False)
    else:
        df_batch.to_csv(csvfile, header=False, index=False)

    print(f"Wrote {len(df_batch)} records to CSV.")

# 6. Main function
def main():
    """Main function to execute the script."""
    # Use the import_data function to get the file
    file_name = import_data()

    # Check if the file is large and handle accordingly
    file_size = os.path.getsize(file_name)
    print(f"File size: {file_size / (1024 * 1024):.2f} MB")

    # Save the output file name
    timestamp = (datetime.now() - timedelta(hours=5)).strftime("%Y%m%d_%H%M%S")
    output_file_name = f"processed_comps_{timestamp}.csv"

    print("Processing file...")
    process_data_stream(file_name, output_file_name)

    print(f"Data has been processed and saved to {output_file_name}.")
    
    # Check if the output file is empty
    if os.path.getsize(output_file_name) == 0:
        print("Error: The output file is empty. Please check the logs for issues during processing.")
    else:
        print(f"Output file size: {os.path.getsize(output_file_name) / 1024:.2f} KB")
    
    # Download the CSV file
    files.download(output_file_name)

if __name__ == "__main__":
    main()
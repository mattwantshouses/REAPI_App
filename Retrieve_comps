# Retrieve Comps
# Version 2.1 - Adding customized parameters to the input
# Future iterations - add lot size parameters

# 1. Imports
import requests
from google.colab import userdata, drive, files
from datetime import datetime
import nest_asyncio
import pytz
import json
import os
from typing import List, Dict, Union
import asyncio
import aiohttp
import pandas as pd
import csv

# Install nest_asyncio if not already installed
!pip install -q nest_asyncio

# Apply the patch 
nest_asyncio.apply()

# 1.05 Define required and optional columns
required_columns = ['data.id', 'data.propertyInfo.address.label']
optional_columns = ['data.propertyInfo.livingSquareFeet', 'data.propertyInfo.bathrooms', 'data.propertyInfo.bedrooms', 'data.propertyInfo.yearBuilt']


# 1.1 Drive Mounting Function
def mount_drive():
    try:
        drive_path = '/content/drive'
        # Check if the drive is already mounted
        if os.path.exists(drive_path) and os.listdir(drive_path):
            print("Google Drive is already mounted.")
            return
        
        # Attempt to mount the drive
        drive.mount(drive_path)
        print("Google Drive mounted successfully.")
    except Exception as e:
        print(f"Error mounting Google Drive: {str(e)}")
        print("Proceeding without Google Drive. Files will only be saved locally.")

# 2. Data Import Function
def import_data(file_path: str = None) -> pd.DataFrame:
    """Import property data from a CSV file."""
    # 2.1 File selection process
    files_in_colab = [f for f in os.listdir() if f.endswith('.csv')]
    
    print("Available CSV files in Colab environment:")
    for i, file in enumerate(files_in_colab, 1):
        print(f"{i}. {file}")
    print(f"{len(files_in_colab) + 1}. Upload your file")
    
    choice = int(input("Enter the number of the file you want to use (or upload option): "))
    
# 2.2 File reading based on selection
    if choice <= len(files_in_colab):
        selected_file = files_in_colab[choice - 1]
    else:
        uploaded = files.upload()
        selected_file = list(uploaded.keys())[0]

    # 2.2.1 Debug information
    print(f"Selected file: {selected_file}")
    print(f"File exists: {os.path.exists(selected_file)}")
    print(f"Full file path: {os.path.abspath(selected_file)}")
    print(f"File size: {os.path.getsize(selected_file)} bytes")

    with open(selected_file, 'r', encoding='utf-8') as f:
        print("First few lines of the file:")
        for i, line in enumerate(f):
            if i < 5:  # Print first 5 lines
                print(line.strip())
            else:
                break

    # 2.3 Preview file contents
    try:
        with open(selected_file, 'r', encoding='utf-8') as f:
            first_lines = [next(f) for _ in range(5)]
        print("First few lines of the file:")
        for line in first_lines:
            print(line.strip())
    except UnicodeDecodeError:
        print("UTF-8 decoding failed, trying with latin-1 encoding")
        with open(selected_file, 'r', encoding='latin-1') as f:
            first_lines = [next(f) for _ in range(5)]
        print("First few lines of the file:")
        for line in first_lines:
            print(line.strip())
    except Exception as e:
        print(f"Error reading file: {str(e)}")
        return None

    # 2.4 Read CSV file into DataFrame
    try:
        df = pd.read_csv(selected_file, encoding='utf-8')
    except UnicodeDecodeError:
        print("UTF-8 decoding failed, trying with latin-1 encoding")
        df = pd.read_csv(selected_file, encoding='latin-1')
    except Exception as e:
        print(f"Error reading CSV: {str(e)}")
        return None

    print(f"Successfully read CSV. Shape: {df.shape}")
    print(f"Columns: {df.columns.tolist()}")
    
    # 2.5 Check for required columns
    missing_required = [col for col in required_columns if col not in df.columns]
    if missing_required:
        print(f"Warning: CSV file is missing required columns: {', '.join(missing_required)}")

    # 2.6 Identify column categories
    present_required = [col for col in required_columns if col in df.columns]
    present_optional = [col for col in optional_columns if col in df.columns]
    extra_columns = [col for col in df.columns if col not in required_columns + optional_columns]

    # 2.7 Report on columns
    print("CSV file analysis:")
    print(f"Present required columns: {', '.join(present_required)}")
    print(f"Present optional columns: {', '.join(present_optional)}")
    print(f"Extra columns: {', '.join(extra_columns)}")
    
    if missing_required:
        print("Warning: Some required columns are missing. This may affect further processing.")
    
    return df

# 3. Comps Retrieval Function
# 3.1 Parameter Calculation Function
def calculate_params(row: pd.Series) -> Dict:
    """Calculate parameters for a single property."""
    if 'data.propertyInfo.address.label' not in row or pd.isna(row['data.propertyInfo.address.label']):
        raise ValueError("Address is not available for this property")

    params = {
        "address": row['data.propertyInfo.address.label'],
        "exact_match": True,
        "max_radius_miles": 1.0,
        "max_days_back": 180,
        "max_results": 10
    }

    if 'data.propertyInfo.livingSquareFeet' in row and pd.notna(row['data.propertyInfo.livingSquareFeet']):
        params.update({
            "living_square_feet_min": max(0, int(row['data.propertyInfo.livingSquareFeet']) - 400),
            "living_square_feet_max": int(row['data.propertyInfo.livingSquareFeet']) + 400
        })
    
    if 'data.propertyInfo.bathrooms' in row and pd.notna(row['data.propertyInfo.bathrooms']):
        params.update({
            "bathrooms_min": max(1, row['data.propertyInfo.bathrooms'] - 1),
            "bathrooms_max": row['data.propertyInfo.bathrooms'] + 1
        })
    
    if 'data.propertyInfo.bedrooms' in row and pd.notna(row['data.propertyInfo.bedrooms']):
        params.update({
            "bedrooms_min": max(1, row['data.propertyInfo.bedrooms'] - 1),
            "bedrooms_max": row['data.propertyInfo.bedrooms'] + 1
        })
    
    if 'data.propertyInfo.yearBuilt' in row and pd.notna(row['data.propertyInfo.yearBuilt']):
        params.update({
            "year_built_min": row['data.propertyInfo.yearBuilt'] - 10,
            "year_built_max": row['data.propertyInfo.yearBuilt'] + 10
        })
    
    return params
# 3.2 Single Comp Retrieval Function
async def get_comp(session, row, headers, url):
    """Asynchronously retrieve comp for a single property."""
    try:
        params = calculate_params(row)
        async with session.post(url, json=params, headers=headers) as response:
            return await response.json()
    except ValueError as e:
        print(f"Error processing row: {e}")
        return {"error": str(e)}

# 3.3 Multiple Comps Retrieval Function
async def get_comps(properties: pd.DataFrame):
    """Retrieve comps for a list of properties and save the results."""
    # API setup
    url = "https://api.realestateapi.com/v3/PropertyComps"
    headers = {
        "accept": "application/json",
        "content-type": "application/json",
        "x-api-key": userdata.get('x-api-key')
    }

    # 3.4 Time setup
    est = pytz.timezone('US/Eastern')
    current_time = datetime.now(est)
    formatted_time = current_time.strftime("%m%d%y_%H%M%S")

    # 3.5 Google Drive mounting
    mount_drive()

    # 3.6 Asynchronous API calls
    async with aiohttp.ClientSession() as session:
        tasks = [get_comp(session, row, headers, url) for _, row in properties.iterrows()]
        results = await asyncio.gather(*tasks)
    
    
    # 3.7 Convert results to DataFrame
    df = pd.DataFrame(results)
    
    # 3.8 Save results
    filename = f"Comps_All_{formatted_time}.json"
    
    # 3.8.1 Save to Google Colab
    with open(filename, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"File '{filename}' has been saved in Colab.")

    # 3.8.2 Try to save to Google Drive if mounted
    drive_base_path = '/content/drive/MyDrive/'
    drive_folder = 'REAPI Comps'  # You can adjust this folder name as needed

    # Create the full path, ensuring it exists
    drive_path = os.path.join(drive_base_path, drive_folder)
    os.makedirs(drive_path, exist_ok=True)

    if os.path.exists(drive_path):
        full_path = os.path.join(drive_path, filename)
        with open(full_path, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"File '{filename}' has been saved in Google Drive folder '{drive_folder}'.")
    else:
        print("Unable to save to Google Drive. File saved only in Colab and downloaded.")

    # 3.8.3 Download to user's machine
    files.download(filename)

    print(f"File '{filename}' has been downloaded to your machine.")

# 4. Main Function
async def main():
    # 4.1 Data import
    properties = import_data()
    if properties is None:
        print("Unable to proceed due to data import error.")
        return
    # 4.2 Comps retrieval
    await get_comps(properties)

# 5. Main Execution
if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    future = asyncio.ensure_future(main())
    loop.run_until_complete(future)